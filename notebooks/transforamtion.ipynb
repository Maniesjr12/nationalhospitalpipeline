{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a97647b-3de6-410a-9f15-f78d210ae431",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "storage_account_name = \"nationalhospitaldatalake\"\n",
    "container_name = \"nationalhospitaletl\"\n",
    "\n",
    "abfss_path = f\"abfss://{container_name}@{storage_account_name}.dfs.core.windows.net/\"\n",
    "\n",
    "\n",
    "# Use it\n",
    "patient_df = spark.read.csv(f'{abfss_path}/raw/patients_data.csv', header=True, inferSchema=True, multiLine= True)\n",
    "\n",
    "display(patient_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a93166eb-02a9-423d-9714-54e87d24335d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "patient_df = spark.read.csv(f'{abfss_path}/raw/patients_data.csv', header=True, inferSchema=True, multiLine= True)\n",
    "\n",
    "imaging_df = spark.read.csv(f'{abfss_path}/raw/imaging_results_data.csv', header=True, inferSchema=True, multiLine= True)\n",
    "\n",
    "lab_df = spark.read.csv(f'{abfss_path}/raw/lab_results_data.csv', header=True, inferSchema=True, multiLine= True)\n",
    "\n",
    "med_records_df = spark.read.csv(f'{abfss_path}/raw/medical_records_data.csv', header=True, inferSchema=True, multiLine= True)\n",
    "\n",
    "trials_df = spark.read.csv(f'{abfss_path}/raw/clinical_trials_data.csv', header=True, inferSchema=True, multiLine= True)\n",
    "\n",
    "participants_df = spark.read.csv(f'{abfss_path}/raw/trial_participants_data.csv', header=True, inferSchema=True, multiLine= True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b74b657e-07e4-468c-a4ce-93a684178d94",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "##cleaning and transformation \n",
    "\n",
    "from pyspark.sql.functions import current_date\n",
    "\n",
    "processed_patients_df = patient_df.dropDuplicates() \\\n",
    "    .na.drop() \\\n",
    "    .withColumn(\"loaded_data\", current_date()) \\\n",
    "    .write.mode(\"overwrite\").csv(f\"{abfss_path}/processed/processed_patients\", header=True)\n",
    "\n",
    "processed_imaging_df = imaging_df.dropDuplicates() \\\n",
    "    .na.drop() \\\n",
    "    .withColumn(\"loaded_data\", current_date()) \\\n",
    "    .write.mode(\"overwrite\").csv(f\"{abfss_path}/processed/processed_imaging\", header=True)\n",
    "\n",
    "processed_lab_df = lab_df.dropDuplicates() \\\n",
    "    .na.drop() \\\n",
    "    .withColumn(\"loaded_data\", current_date()) \\\n",
    "    .write.mode(\"overwrite\").csv(f\"{abfss_path}/processed/processed_lab\", header=True)\n",
    "\n",
    "processed_med_records_df = med_records_df.dropDuplicates() \\\n",
    "    .na.drop() \\\n",
    "    .withColumn(\"loaded_data\", current_date()) \\\n",
    "    .write.mode(\"overwrite\").csv(f\"{abfss_path}/processed/processed_med_records\", header=True)\n",
    "\n",
    "processed_trials_df = trials_df.dropDuplicates() \\\n",
    "    .na.drop() \\\n",
    "    .withColumn(\"loaded_data\", current_date()) \\\n",
    "    .write.mode(\"overwrite\").csv(f\"{abfss_path}/processed/processed_trials\", header=True)\n",
    "\n",
    "processed_participants_df = participants_df.dropDuplicates() \\\n",
    "    .na.drop() \\\n",
    "    .withColumn(\"loaded_data\", current_date()) \\\n",
    "    .write.mode(\"overwrite\").csv(f\"{abfss_path}/processed/processed_participants\", header=True)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "gold",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}